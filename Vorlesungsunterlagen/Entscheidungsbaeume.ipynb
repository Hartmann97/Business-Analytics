{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f97b402f-53fe-4ba7-b769-49e31f339bbf",
   "metadata": {},
   "source": [
    "#### Business Analytics FHDW 2025\n",
    "# Klassifikations- und Regressionsbäume\n",
    "## Konstruktion eines Klassifikationsbaums\n",
    "\n",
    "Für den Aufbau eines Klassifikationsbaums betrachten wir noch einmal das Beispiel der Sitzrasenmäher. Der Hersteller möchte die potentiellen Käufer klassifizieren. Wir lesen den Datensatz `RidingMowers`erneut ein und teilen ihn auf in die Eigentümer und Nichteigentümer.\n",
    "\n",
    "Für die Baumdarstellung, falls noch nicht installiert:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856f810b-3759-42d4-898c-acd7d7055018",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "! conda install --yes --prefix {sys.prefix} -c conda-forge pydotplus\n",
    "# Bei Bedarf auch noch conda install -c conda-forge graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffad9da-7965-4cb6-be54-e36374b5ab50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "from dmba import plotDecisionTree, classificationSummary, regressionSummary\n",
    "\n",
    "mower_df = pd.read_csv('./Daten/RidingMowers.csv')\n",
    "subset_owners = mower_df[mower_df['Ownership']=='Owner']\n",
    "subset_nonowners = mower_df[mower_df['Ownership']=='Nonowner']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cd5433-196c-42b7-87de-3005b2349aac",
   "metadata": {},
   "source": [
    "Beide Gruppen stellen wir grafisch dar. Da wir den Plot ein paar Mal brauchen, machen wir eine Funktion `create_example_plot` daraus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e348cd67-2ba2-434d-945e-99f710cbdb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_example_plot():\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(subset_owners.Income, subset_owners.Lot_Size, marker='o',\n",
    "           label='Eigentümer', color='C0', s=30)\n",
    "    ax.scatter(subset_nonowners.Income, subset_nonowners.Lot_Size, marker='o',\n",
    "           label='Nichteigentümer', color='C1', facecolors='none', s=30)\n",
    "\n",
    "    plt.xlabel('Einkommen')\n",
    "    plt.ylabel('Grundstücksgröße')\n",
    "    ax.set_xlim(20, 120)\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    ax.legend(handles, labels, loc=4)\n",
    "    \n",
    "    return ax\n",
    "\n",
    "ax = create_example_plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2965968-8e02-41fd-a3d3-f84f0e3e7cc4",
   "metadata": {},
   "source": [
    "Nun wollen wir diesen Prädiktorraum *rekursiv partitionieren*. D. h. wir teilen den $p$-dimensionalen Raum der Variablen $X_1,..., X_p$ auf in nicht-überlappende, multidimensionale Rechtecke. Die Prädiktorvariablen können hier kontinuierlich, binär oder ordinal sein. Die Rekursion bezieht sich darauf, aufgeteilte Rechtecke wieder weiter aufzuteilen. Das Kriterium dafür ist, Rechtecke mit möglichst homogenen Gruppen zu bilden, also Datensätzen der gleichen Klasse.\n",
    "\n",
    "Um die Heterogenität in einem solchen Bereich $A$ zu messen, bietet sich der *Gini-Index* an: $I(A)=1-\\sum\\limits_{k=1}^{m}{p^2_k}$ mit $p^2_k$ dem Anteil der Datensätze der Klasse $k$ in $A$. Dieses Maß ist $0$, wenn alle Datensätze zu einer Klasse gehören und $(m-1)/m$, wenn alle $m$ Klassen zu gleichen Anteilen enthalten sind, die Heterogenität also maximal ist.\n",
    "\n",
    "In unserem Beispiel besitzt die Ausgangsdatenmenge gleiche Anteile von Eigentümern und Nichteigentümern, der Gini-Index ist also maximal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ff099b-07de-463c-9b7e-387dff7f141c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_index(data, k_name):\n",
    "    return 1-np.sum([(len(data[data[k_name]==k])/len(data))**2 for k in set(data[k_name])])\n",
    "\n",
    "print('Gini-Index der Ausgangsdatenmenge = {}'.format(gini_index(mower_df, 'Ownership')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65cea28-1217-4cd3-87bf-d48dae4d8b63",
   "metadata": {},
   "source": [
    "Ein Konstruktionsalgorithmus für Klassifikationsbäume betrachtet alle Prädiktorvariablen, hier *Grundstücksgröße* und *Einkommen*, und alle möglichen Aufteilungspunkte, die einfach die Mittelpunkte zwischen zwei aufeinander folgenden Werten sind. Für jeden Aufteilungspunkt wird der Heterogenitätsgrad ermittelt und daraus ein Ranking erzeugt. Der Punkt mit der höchsten Heterogenitätsreduktion wird gewählt.\n",
    "\n",
    "Im gegebenen Beispiel findet die erste Aufteilung an $59.7$ auf der Einkommensachse statt. Der Raum $(X_1, X_2)$ teilt sich also in zwei Rechtecke mit Einkommen $\\ge 59.7$ und $< 59.7$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a4cae8-6a57-498b-aaaf-93115ca57c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = create_example_plot()\n",
    "ax.plot([59.7, 59.7], [ax.get_ylim()[0], ax.get_ylim()[1]], color='grey')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0babcc90-fbe9-4b38-b6c6-8c049b766e91",
   "metadata": {},
   "source": [
    "Ermitteln wir beispielhaft den Gini-Index des ersten Rechtecks, das aus einem Eigentümer und sieben Nichteigentümern besteht, sehen wir die reduzierte Heterogenität."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba57467e-88a3-439b-b43b-5d219c846adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_rectangle = mower_df[mower_df.Income <= 59.7]\n",
    "print('Gini-Index der ersten Aufteilung = {}'.format(gini_index(first_rectangle, 'Ownership')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e605e293-93de-47ad-ab67-5d431158ac7e",
   "metadata": {},
   "source": [
    "Der Klassifikationsbaumalgorithmus nutzt diese Aufteilung nun, um zu einem Knoten zwei Nachfolgerknoten zu erzeugen. Wir lassen den `DecisionTreeClassifier` von *scikit-learn* diesen ersten Schritt ausführen (also einen Baum der Tiefe 1 erzeugen) und stellen das Ergebnis dar. Grundlage für die Anpassung durch `fit` ist hier der gesamte Datensatz mit der Zielvariable *Eigentum/Ownership*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083a45af-32eb-4062-91e3-7e1208393eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_tree = DecisionTreeClassifier(random_state=0, max_depth=1)\n",
    "class_tree.fit(mower_df.drop(columns=['Ownership']), mower_df['Ownership'])\n",
    "print('Classes: {}'.format(', '.join(class_tree.classes_)))\n",
    "plotDecisionTree(class_tree, feature_names=mower_df.columns[:2],\n",
    "                 class_names=class_tree.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9087fab4-ad8a-4284-b4fe-714b467a933b",
   "metadata": {},
   "source": [
    "Im nächsten Schritt teilen wir das erste Rechteck weiter auf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90bf254-4f04-4a46-b275-15713bd969aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = create_example_plot()\n",
    "ax.plot([59.7, 59.7], [ax.get_ylim()[0], ax.get_ylim()[1]], color='grey')\n",
    "ax.plot([ax.get_xlim()[0], 59.7], [21.4, 21.4], color='grey')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccc78dd-963f-416e-baa7-534e52d1241b",
   "metadata": {},
   "source": [
    "Nach Aufteilung auch des zweiten Rechtecks (ohne Abbildung) entsteht folgender Baum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85157a4d-c553-4b2d-92b6-648edf357771",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_tree = DecisionTreeClassifier(random_state=0, max_depth=2)\n",
    "class_tree.fit(mower_df.drop(columns=['Ownership']), mower_df['Ownership'])\n",
    "plotDecisionTree(class_tree, feature_names=mower_df.columns[:2],\n",
    "                 class_names=class_tree.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476b0770-d1b7-441d-aab4-108447e212ea",
   "metadata": {},
   "source": [
    "Die vollständige rekursive Aufteilung sieht dann so aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f99721-83ed-4fca-8ef5-b5baed0a5cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = create_example_plot()\n",
    "ax.plot([59.7, 59.7], [ax.get_ylim()[0], ax.get_ylim()[1]], color='grey')\n",
    "ax.plot([ax.get_xlim()[0], 59.7], [21.4, 21.4], color='grey')\n",
    "ax.plot([59.7, ax.get_xlim()[1]], [19.8, 19.8], color='grey')\n",
    "ax.plot([84.75, 84.75], [ax.get_ylim()[0], 19.8], color='grey')\n",
    "ax.plot([61.5, 61.5], [ax.get_ylim()[0], 19.8], color='grey')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35ef8a4-4583-4c46-b29e-414f29622e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_tree = DecisionTreeClassifier(random_state=0)\n",
    "class_tree.fit(mower_df.drop(columns=['Ownership']), mower_df['Ownership'])\n",
    "plotDecisionTree(class_tree, feature_names=mower_df.columns[:2],\n",
    "                 class_names=class_tree.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d6c614-93bd-4163-ade9-ee199cc62e67",
   "metadata": {},
   "source": [
    "## Performance eines Klassifikationsbaums\n",
    "\n",
    "Wie wir schon bei den bisher betrachteten Ansätzen gelernt haben, reicht die bloße Anpassung unseres Modells an einen Trainingsdatensatz nicht aus, um zu (statistisch) belastbaren Ergebnissen zu kommen. Das Modell muss sich mit Validierungsdaten beweisen. Für die hier behandelten Bäume gilt das um so mehr: Mit unterschiedlichen Auswahlen von Datensätzen kann sich die Baumstruktur als recht instabil zeigen, außerdem neigen vollständig angepasste Bäume zu Überanpassung.\n",
    "\n",
    "### Instabilität\n",
    "\n",
    "Wir betrachten als weiteres Beispiel `UniversalBank` und die Akzeptanz eines persönlichen Darlehens im Kundenstamm. Die Daten zeigen uns die Ergebnisse einer vorhergegangenen Marketingkampagne mit einer Erfolgsquote von über 9%. Ein Klassifikationsbaum liefert nun einen plausiblen Ansatz zur Analyse der einzelnen Faktoren, die zum Erfolg und der Vergabe eines Darlehens führen und damit ein Modell des Kundenverhaltens bilden.\n",
    "\n",
    "Zunächst konstruieren wir den vollständigen Baum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9841a2-3c1c-437d-ab10-f92ca0c2bcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_df = pd.read_csv('./Daten/UniversalBank.csv')\n",
    "bank_df\n",
    "bank_df.drop(columns=['ID', 'ZIP Code'], inplace=True)\n",
    "\n",
    "X = bank_df.drop(columns=['PersonalLoan'])\n",
    "y = bank_df['PersonalLoan']\n",
    "train_X, valid_X, train_y, valid_y = train_test_split(X, y, test_size=0.4, random_state=1)\n",
    "\n",
    "full_class_tree = DecisionTreeClassifier(random_state=1)\n",
    "full_class_tree.fit(train_X, train_y)\n",
    "\n",
    "plotDecisionTree(full_class_tree, feature_names=train_X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fb654d-88ae-45dd-b00e-52ba98f7c223",
   "metadata": {},
   "source": [
    "Im Klassifikationsbaum führen von 43 Endknoten 24 zum abgelehnten und 19 zum angenommenen Darlehen. Zunächst generieren wir die Konfusionsmatrizen für die Trainings- und Validierungsdaten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3c8eec-ea15-4a4f-af0a-3f437bc800ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "classificationSummary(train_y, full_class_tree.predict(train_X))\n",
    "print()\n",
    "classificationSummary(valid_y, full_class_tree.predict(valid_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ca8279-9772-4384-85fe-49b524e3be69",
   "metadata": {},
   "source": [
    "Wie zu erwarten, werden die Trainingsdaten genau klassifiziert, die Validierungsdaten allerdings nur mit einer Genauigkeit von knapp 98%.\n",
    "\n",
    "Im Falle des Baums können diese Ergebnisse je nach Auswahl der Datensätze stark variieren. Daher ist hier eine *Kreuzvalidierung* der plausiblere Ansatz für die Prüfung der Performance, um direkt einen Eindruck dieser Schwankungen bzw. eine Sensitivitätsanalyse gegenüber verschiedenen Datenpartitionen zu erhalten. Eine *k-fold cross-validation* partitioniert die Daten in $k$ nicht-überlappende Stichproben (*folds*; \"Faltungen\"). Das Modell wird dann $k$-mal angepasst. In jedem Durchlauf ist eine der Stichproben der Validierungsdatensatz, die restlichen $k-1$ Stichproben sind die Trainingsdaten. Üblich sind $k=5$ cross-validations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91611ca-7c85-486a-a4ed-d6fcff6ccf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_classifier = DecisionTreeClassifier(random_state=1)\n",
    "# Das Fitting macht die folgende Funktion selbst; ist hier also nicht nötig.\n",
    "scores = cross_val_score(tree_classifier, train_X, train_y, cv=5)\n",
    "print('Genauigkeit über jede Stichprobe: ', [f'{acc:.3f}' for acc in scores])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd759f1-13da-4bd9-b050-33dbf8469f72",
   "metadata": {},
   "source": [
    "### Überanpassung\n",
    "\n",
    "Bei der oben gezeigten Konstruktion eines Baums erwarten wir, dass mit jedem Wachstumsschritt die Genauigkeit der Klassifikation zunimmt. Für die Trainingsdaten trifft das bis zum maximalen Ausbau auch zu. Für neue Datensätze ist allerdings i. d. R. bereits früher ein Punkt erreicht, an dem die Beziehung zwischen Prädiktoren und Klasse vollständig (oder so vollständig wie auf Basis der Daten möglich) modelliert ist. Ab diesem Punkt beginnt der Baum, das Rauschen der Trainingsdaten zu modellieren und der Fehler nimmt wieder zu. Eine intuitive Erklärung bei sehr großen Bäumen sind die irgendwann sehr kleinen Stichproben/Samples der letzten Entscheidungsknoten.\n",
    "\n",
    "Es gibt eine Reihe von nahe liegenden Ansätzen, das Wachstum zu begrenzen: \n",
    "- Maximale Tiefe, \n",
    "- minimale Anzahl von Datensätzen in Endknoten, \n",
    "- minimale Reduktion von Heterogenität durch einen Aufteilungsschritt. \n",
    "\n",
    "Zu diesen Parametern gibt es aber keine einfachen Regeln oder belastbare allgemeine Heuristiken, um ihren Wert zu bestimmen.\n",
    "\n",
    "Da wir sie im `DecisionTreeClassifier` zur Steuerung der Modellkonstruktion nutzen können, ein Beispiel mit einem beschränkten Baum über die Trainingsdaten aus `UniversalBank`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64084899-7243-46ea-95a6-363eb3701b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mit den Parametern können Sie gut selbst experimentieren:\n",
    "small_class_tree = DecisionTreeClassifier(max_depth=30, \n",
    "                                          min_samples_split=20, \n",
    "                                          min_impurity_decrease=0.01, \n",
    "                                          random_state=1)\n",
    "small_class_tree.fit(train_X, train_y)\n",
    "\n",
    "plotDecisionTree(small_class_tree, feature_names=train_X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c091088-d6e0-4095-859d-a8d36849472f",
   "metadata": {},
   "outputs": [],
   "source": [
    "classificationSummary(train_y, small_class_tree.predict(train_X))\n",
    "print()\n",
    "classificationSummary(valid_y, small_class_tree.predict(valid_X))\n",
    "print()\n",
    "scores = cross_val_score(small_class_tree, train_X, train_y, cv=5)\n",
    "print('Genauigkeit über jede Stichprobe: ', [f'{acc:.3f}' for acc in scores])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57accb00-a742-4a11-a1cb-083cdba3d62f",
   "metadata": {},
   "source": [
    "Der reduzierte Baum bietet - für neue Datensätze - eine ähnliche Qualität der Klassifikation wie der unbeschränkte Baum. \n",
    "\n",
    "Auch, wenn wir für die beschränkenden Parameter nicht einfach die optimalen Werte berechnen können, hilft uns auch hier wieder der Rechner durch *brute force*: Eine *Gittersuche* (*grid search*) ermittelt aus verschiedenen Wertekombinationen die, die zum Baum mit dem kleinsten Fehler führt. Damit dabei nicht eine Überanpassung an Trainings- oder Validierungsdaten erfolgt, führt die Suche zunächst *cross-validation* (s. o.) auf die Trainingsdaten aus und evaluiert dann die Performance mit den Validierungsdaten.\n",
    "\n",
    "Eine erschöpfende Gittersuche ist implementiert in `GridSearchCV`. Sie kann schnell rechenintensiv werden. Wir fangen an mit einer initialen Schätzung der Parameter. Dieser erste Durchlauf erfordert $4 \\cdot 5 \\cdot 5 = 100$ Kombinationen. Bei mehr Parametern steigt der Aufwand entsprechend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93991b89-2008-4f37-a7c1-fe1bbe02d04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'max_depth': [10, 20, 30, 40],\n",
    "    'min_impurity_decrease': [0, 0.0005, 0.001, 0.005, 0.01],\n",
    "    'min_samples_split': [20, 40, 60, 80, 100]\n",
    "}\n",
    "grid_search = GridSearchCV(DecisionTreeClassifier(random_state=1), param_grid, cv=5, n_jobs=-1)\n",
    "grid_search.fit(train_X, train_y)\n",
    "print('Initiale Bewertung: ', grid_search.best_score_)\n",
    "print('Initiale Parameter: ', grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68317177-4add-4ba9-9560-23200e516883",
   "metadata": {},
   "source": [
    "Die resultierenden Werte liefern uns Hinweise, in welche Richtungen eine Verfeinerung suchen sollte:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc31911-bea4-42e3-8f8d-04e04d2b3c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'max_depth': list(range(5, 15)),\n",
    "    'min_impurity_decrease': [0.0001, 0.0005, 0.001],\n",
    "    'min_samples_split': list(range(10, 30))\n",
    "}\n",
    "grid_search = GridSearchCV(DecisionTreeClassifier(random_state=1), param_grid, cv=5, n_jobs=-1)\n",
    "grid_search.fit(train_X, train_y)\n",
    "print('Verbesserte Bewertung: ', grid_search.best_score_)\n",
    "print('Verbesserte Parameter: ', grid_search.best_params_)\n",
    "\n",
    "best_class_tree = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98465d33-7965-410a-a18b-b1ab365eed3a",
   "metadata": {},
   "source": [
    "So bekommen wir einen weiter verbesserten Baum mit folgender Performance und Form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af0ec88-0d36-451a-b8ba-2ff6f3a9b0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "classificationSummary(train_y, best_class_tree.predict(train_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8118c379-a449-4ed2-b781-65075c074a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "classificationSummary(valid_y, best_class_tree.predict(valid_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0875006-d1ac-4334-9db4-abf0b22027e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotDecisionTree(best_class_tree, feature_names=train_X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6b1b12-cd6f-4499-b358-6a1b9ea95054",
   "metadata": {},
   "source": [
    "## Aufgabe\n",
    "\n",
    "Wir betrachten noch einmal den Datensatz *FlightDelays.csv*.\n",
    "\n",
    "1. Interaktiv: Was sind hier sinnvolle Prädiktoren für die Verspätung von Flügen?\n",
    "\n",
    "2. Bereiten Sie den Datensatz für die Generierung von Entscheidungsbäumen vor: Wandeln Sie die erforderlichen Variablen in kategorische um, erzeugen Sie Dummy-Variablen und teilen Sie 60% Trainingsdaten und 40% Validierungsdaten auf.\n",
    "\n",
    "3. Generieren Sie einen Entscheidungsbaum mit einer maximalen Tiefe von 8 zur Vorhersage von Verspätungen und plotten Sie ihn. Lassen sich daraus einfache praktische Regeln für die Vorhersage von Verspätungen ableiten?\n",
    "\n",
    "4. Beurteilen Sie die Qualität der Vorhersage und vergleichen Sie diese mit den Ergebnissen der naiven Bayes-Klassifikation (Konfusionsmatrizen und Kreuzvalidierung). Verändern Sie die maximale Tiefe des Baums und beurteilen Sie die Auswirkungen auf die Prädiktionen (besser, schlechter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cfb2f3-e5c0-4ce9-a125-a07234d00a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "delays_df = pd.read_csv('./Daten/FlightDelays.csv')\n",
    "delays_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3729aef-a5c0-403f-b1cb-d5646d76a8ce",
   "metadata": {},
   "source": [
    "## Regressionsbäume\n",
    "\n",
    "Eine numerische Zielvariable erhalten wir mit *Regressionsbäumen*. Das Konstruktionsprinzip bleibt hier das gleiche, wie bei der Klassifikation: Der Algorithmus minimiert die Heterogenität unterschiedlicher Aufteilungen des Prädiktorraums.\n",
    "\n",
    "Zur Veranschaulichung der Unterschiede zwischen Regressions- und Klassifikationsbäumen betrachten wir noch einmal das Beispiel der Gebrauchtwagenbewertung mit dem Datensatz `ToyotaCorolla`. Wir generieren uns mit dem oben gezeigten Vorgehen einen reduzierten Baum dazu. Dieses Mal nutzen wir aber den `DecisionTreeRegressor`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a1f296-a1da-4e25-8862-7367db8e5c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "toyota_corolla_df = pd.read_csv('./Daten/ToyotaCorolla.csv').iloc[:1000,:]\n",
    "toyota_corolla_df = toyota_corolla_df.rename(columns={'age_08_04': 'age', 'quarterly_tax': 'tax'})\n",
    "\n",
    "predictors = ['age', 'km', 'fuel_type', 'hp', 'met_color', 'automatic', 'cc', 'doors', 'tax', 'weight']\n",
    "outcome = 'price'\n",
    "\n",
    "X = pd.get_dummies(toyota_corolla_df[predictors], drop_first=True)\n",
    "y = toyota_corolla_df[outcome]\n",
    "\n",
    "train_X, valid_X, train_y, valid_y = train_test_split(X, y, test_size=0.4, random_state=1)\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [5, 10, 15, 20, 25],\n",
    "    'min_impurity_decrease': [0, 0.001, 0.005, 0.01],\n",
    "    'min_samples_split': [10, 20, 30, 40, 50]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(DecisionTreeRegressor(), param_grid, cv=5, n_jobs=-1)\n",
    "grid_search.fit(train_X, train_y)\n",
    "print('Initiale Parameter: ', grid_search.best_params_)\n",
    "regression_tree = grid_search.best_estimator_\n",
    "regressionSummary(valid_y, regression_tree.predict(valid_X))\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
    "    'min_impurity_decrease': [0, 0.001, 0.002, 0.003, 0.005, 0.006, 0.007, 0.008],\n",
    "    'min_samples_split': [14, 15, 16, 18, 20]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(DecisionTreeRegressor(), param_grid, cv=5, n_jobs=-1)\n",
    "grid_search.fit(train_X, train_y)\n",
    "print()\n",
    "print('Verbesserte Parameter: ', grid_search.best_params_)\n",
    "regression_tree = grid_search.best_estimator_\n",
    "regressionSummary(valid_y, regression_tree.predict(valid_X))\n",
    "\n",
    "plotDecisionTree(regression_tree, feature_names=train_X.columns, rotate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49218d97-c389-416f-b1d2-69c1f31eb907",
   "metadata": {},
   "source": [
    "### Prädiktion\n",
    "\n",
    "Zunächst sehen wir in den oberen Ebenen des Baums, welche Einflussfaktoren bzw. Prädiktoren beim Preis die größte Rolle spielen. Möchten wir den Preis eines Fahrzeugs mit z. B. einem *Alter von 60, Kilometerstand von 160.000 und 100 PS* voraussagen, erreichen wir den Endknoten mit dem Wert *8392.857*. Die Klassifikation oben wurde durch die Mehrheit der Trainingsdatensätze erreicht, die im Endknoten ausgewertet wurden. Die Regression ermittelt statt dessen den numerischen Zielwert aus den Durchschnittswerten dieser Trainingsdatensätze - im Fall hier ist 8.392,86 der Durchschnittspreis der sieben Fahrzeuge aus den Trainingsdaten, die über 49.5 Monate alt sind, einen Kilometerstand über 128.361 und über 93.5 PS haben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9307282-18e6-44c0-a778-e37a83ff450a",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_cars = train_X[(train_X['age']>=49.5) \n",
    "                      & (train_X['km']>=128361)\n",
    "                      & (train_X['hp']>=93.5)].join(train_y)\n",
    "node_mean = subset_cars['price'].mean()\n",
    "print(node_mean)\n",
    "subset_cars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc12a753-e946-4cc2-8a8e-a8b2895c7251",
   "metadata": {},
   "source": [
    "### Heterogenität\n",
    "\n",
    "Bei der Klassifikation liefert der *Gini-Index* das Verhältnis zwischen Klassen von Datensätzen in einem Knoten. Bei der Regression ist ein typisches Heterogenitätsmaß die Summe der quadrierten Abweichungen vom Mittelwert eines Endknotens. Da dieses Mittel die Prädiktion repräsentiert, entspricht das der bekannten Summe der Fehlerquadrate. Für das Beispiel oben:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5005f28c-8a87-49ea-83b7-ab900f9e3357",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, car in subset_cars.iterrows():\n",
    "    print('(Preis {:.2f} - {:.2f})^2 = {:.2f}'.format(car['price'], node_mean, (car['price']-node_mean)**2))\n",
    "\n",
    "# Oder einfach so:\n",
    "sum((subset_cars['price']-node_mean)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d312ef4b-c7f8-4188-8ad9-9545b5d87611",
   "metadata": {},
   "source": [
    "### Performance\n",
    "\n",
    "Wie wir oben schon bei der Konstruktion des Regressionsbaums gesehen haben, gelten für die Beurteilung der Vorhersagequalität hier die gleichen Größen wie bei anderen Regressionen, z. B. der linearen, also Fehlersummen wie RMSE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f7c529-00cb-443c-9d62-8748f63fecf8",
   "metadata": {},
   "source": [
    "## Verbesserung der Vorhersage: Random Forests und Boosted Trees\n",
    "\n",
    "Die Nutzung eines einzelnen Baums trägt durch die gute Darstellbarkeit - wie wir oben gesehen haben - sehr zur Transparenz und Nachvollziehbarkeit dieser Methode bei. Wenn das jedoch zu Gunsten der Performance vernachlässigt werden kann, können wir *Ensembles* aus mehreren Bäumen bilden. Die folgenden Ansätze sind sowohl für die Klassifikation, als auch die Regression nutzbar (`Regressor` statt `Classifier` in den Bibliotheken).\n",
    "\n",
    "### Random Forests\n",
    "\n",
    "Das grundsätzliche Vorgehen sieht bei diesen \"zufälligen Wäldern\" so aus:\n",
    "\n",
    "1. Mehrere zufällige Stichproben mit Zurücklegen aus den Daten ziehen (*bootstrap*).\n",
    "2. Jede Stichprobe mit einer zufälligen Auswahl an Prädiktoren an einen Klassifikations- oder Regressionsbaum anpassen (so entsteht der Wald).\n",
    "3. Aus den Vorhersagen der einzelnen Bäume die Mehrheitsentscheidungen für Klassifikation nutzen bzw. die Durchschnitte für Regression bilden. Aus diesen Kombinationen erhoffen wir uns verbesserte Vorhersagen.\n",
    "\n",
    "Wenden wir diese Methode auf unser Bank-Szenario an (`n_estimators` ist die Anzahl der Bäume des Waldes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a693897-6b99-4bd7-a67f-5915f0d357a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "bank_df = pd.read_csv('./Daten/UniversalBank.csv')\n",
    "bank_df\n",
    "bank_df.drop(columns=['ID', 'ZIP Code'], inplace=True)\n",
    "\n",
    "X = bank_df.drop(columns=['PersonalLoan'])\n",
    "y = bank_df['PersonalLoan']\n",
    "train_X, valid_X, train_y, valid_y = train_test_split(X, y, test_size=0.4, random_state=1)\n",
    "\n",
    "random_forest = RandomForestClassifier(n_estimators=500, random_state=1)\n",
    "random_forest.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98e1abf-e611-415e-ba65-116185abf92d",
   "metadata": {},
   "source": [
    "Wir können hier keine, oder zumindest keine übersichtlichen, Baumdiagramme mehr zur Visualisierung nutzen. Die Wälder liefern aber Bewertungen der Wichtigkeit einzelner Variablen: Das ist hier ihr relativer Beitrag in Form der summierten Abnahme des Gini-Index für diesen Prädiktor über alle Bäume im Wald.\n",
    "\n",
    "Für unser Beispiel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ae046f-4e74-499b-bfd1-543543abaabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = random_forest.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in random_forest.estimators_], axis=0)\n",
    "\n",
    "df = pd.DataFrame({'Eigenschaft': train_X.columns, 'Wichtigkeit': importances, 'Std': std})\n",
    "df = df.sort_values('Wichtigkeit', ascending=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697894fc-dd9a-406f-aff5-0db0ddda568e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values('Wichtigkeit')\n",
    "ax = df.plot(kind='barh', xerr='Std', x='Eigenschaft', legend=False)\n",
    "ax.set_ylabel('')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44df9c53-bf2d-4201-9196-14d39d189b41",
   "metadata": {},
   "source": [
    "Der *Random Forest* liefert uns für das gegebene Szenario eine ähnliche Genauigkeit der Vorhersage der Validierungsdaten wie die *Gittersuche* oben:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b101b10-7c25-4503-99b1-c94a8e049896",
   "metadata": {},
   "outputs": [],
   "source": [
    "classificationSummary(valid_y, random_forest.predict(valid_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553b7aeb-0459-4032-b948-5ddd47d62b53",
   "metadata": {},
   "source": [
    "### Boosted Trees\n",
    "\n",
    "Eine weitere Möglichkeit, mehrere Bäume für die Vorhersage zu nutzen, liegt in einer Sequenz aus Bäumen, in der jeder Folgebaum die Fehlklassifikationen des Vorgängers kompensieren soll:\n",
    "\n",
    "1. Anpassen eines einzelnen Baumes.\n",
    "2. Dazu eine Stichprobe finden, in der die Wahrscheinlichkeit für fehlklassifizierte Datensätze hoch ist.\n",
    "3. Einen Folgebaum auf diese Stichprobe anpassen.\n",
    "4. Die Schritte 2 und 3 mehrfach wiederholen.\n",
    "5. Aus den gewichteten Ergebnissen die Vorhersagen generieren, mit größerem Gewicht auf hintere Bäume in der Sequenz.\n",
    "\n",
    "Wenden wir auch diesen Ansatz auf unser Beispiel an:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a25721-4dbc-4028-a837-9db9bb6cb017",
   "metadata": {},
   "outputs": [],
   "source": [
    "boosted = GradientBoostingClassifier()\n",
    "boosted.fit(train_X, train_y)\n",
    "classificationSummary(valid_y, boosted.predict(valid_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7f23b0-2b08-43c4-a80e-4bc95140385c",
   "metadata": {},
   "source": [
    "Wir erhalten eine weiter leicht verbesserte Genauigkeit der Vorhersage. \n",
    "\n",
    "Insbesondere ist aber der Fehler in der Klassifikation der *1* reduziert. Dies ist eine besondere Eigenschaft der *Boosted Trees*: Die einfachen Klassifizierer lassen sich stark von dominierenden Klassen beeinflussen (*0* macht im gegebenen Datensatz über 90% aus). Entsprechend gibt es relativ viele Fehlklassifikationen der *1* in einem einzelnen Baum. Der oben kurz beschriebene Boosting-Algorithmus fokussiert sich aber genau auf diese Fehlklassifikationen und hat daher gute Chancen, sie zu reduzieren."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5432849-72ad-4dd6-9d0f-7d930f6b1702",
   "metadata": {},
   "source": [
    "## Aufgabe\n",
    "\n",
    "Versuchen Sie, mit *Random Forests* und *Boosted Trees* die Genauigkeit des Vorhersagemodells für die Flugverspätungen aus der letzten Aufgabe zu verbessern."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
