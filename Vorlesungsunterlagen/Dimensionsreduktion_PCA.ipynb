{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6deb209-9f80-41b7-b810-9456365cdec8",
   "metadata": {},
   "source": [
    "#### Business Analytics FHDW 2024\n",
    "# Dimensionsreduktion mit der Principal Components Analysis\n",
    "## am Beispiel von Daten über Frühstücksflocken\n",
    "\n",
    "Das Beispiel ist aus Shmueli et al.; für unsere Veranstaltung und Zwecke ein wenig umgebaut und aufbereitet. Wir brauchen neben den schon vorgestellten Hilfsmitteln und einer kleinen Auswertungsfunktion `pcaSummary` wieder den Datensatz *cereal.csv*, den wir in ein `DataFrame` einlesen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4df32cd-32c7-40c8-be29-e64a03cbca5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "\n",
    "def pcaSummary(pca):\n",
    "    return pd.DataFrame({'Varianz':pca.explained_variance_,\n",
    "                         'Kumulierte Varianz':np.cumsum(pca.explained_variance_),\n",
    "                         'Varianzanteil':pca.explained_variance_ratio_,\n",
    "                         'Kumulierte Anteile':np.cumsum(pca.explained_variance_ratio_)})\n",
    "\n",
    "cereals_df = pd.read_csv('./Daten/cereal.csv')\n",
    "cereals_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c779822-33e2-4cbe-8a2d-99683ffb2c4b",
   "metadata": {},
   "source": [
    "Zunächst interessieren uns nur zwei Variablen aus dem Datensatz: Kalorien (*calories*) und Bewertung (*rating*). Hängen die irgendwie zusammen? Die Korrelationsmatrix eines Dataframe kennen wir ja schon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a27659a-4524-4817-80b5-f4274a4504ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "cereals_df[['calories', 'rating']].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37aea58-1ea9-4f33-ab12-76614e1a3441",
   "metadata": {},
   "source": [
    "Kurze Erinnerung: Nach unserer Vorlesung DAML ist das eine starke (negative) Korrelation zwischen den beiden Variablen.\n",
    "\n",
    "$0.1 \\leq |r| \\lt 0.3 \\to$ schwach\n",
    "\n",
    "$0.3 \\leq |r| \\lt 0.5 \\to$ moderat\n",
    "\n",
    "$0.5 \\leq |r| \\leq 1 \\to$ stark\n",
    "\n",
    "Wenn also ein Produkt viele Kalorien hat, wird es niedrig bewertet. Hoch bewertete Produkte haben hingegen wenig Kalorien.\n",
    "\n",
    "Für uns sind hier die Varianzen der Variablen eine Größe für ihren Informationsgehalt. Schauen wir uns die Kovarianzen der beiden Variablen an und setzen sie in Verhältnis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba36330a-f7f1-47d6-b6c7-d427368bbc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "cereals_df[['calories', 'rating']].cov()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ddbabf-e36a-4bfd-8e21-5a8bf584525e",
   "metadata": {},
   "outputs": [],
   "source": [
    "variance_calories = cereals_df.calories.var()\n",
    "variance_rating = cereals_df.rating.var()\n",
    "variance_overall = variance_calories + variance_rating\n",
    " \n",
    "print(f'Varianz Kalorien = {variance_calories:.4f}, Varianz Bewertung = {variance_rating:.4f}, Summe Varianz = {variance_overall:.4f}')\n",
    "print(f'Verhältnis Varianz Kalorien / Summe Varianzen = {(variance_calories / variance_overall):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9786b0b1-0fd8-4931-9ed3-a35beaf6121e",
   "metadata": {},
   "source": [
    "Die Daten über Kalorien sind also für knapp 66% der Varianz insgesamt verantwortlich. Würden wir *rating* nun entfernen, gingen uns 34% der Information über die Varianz insgesamt verloren. Das ist viel, daher wäre unser händischer Ansatz aus dem Abschnitt *Korrelationsanalyse* hier nicht ideal. Werfen wir einen Blick auf die Daten, ihre Eigenvektoren und deren Eigenwerte. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220f49e7-890b-4800-92b4-a5a3e455a6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_1 = cereals_df.plot.scatter(x='calories', y='rating')\n",
    "\n",
    "cov_matrix = cereals_df[['calories', 'rating']].cov()\n",
    "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "\n",
    "print(f'Der erste Eigenvektor {eigenvectors[:,0]} hat den Eigenwert {eigenvalues[0]:.4f}')\n",
    "print(f'Der zweite Eigenvektor {eigenvectors[:,1]} hat den Eigenwert {eigenvalues[1]:.4f}')\n",
    "\n",
    "calories_mean = cereals_df.calories.mean()\n",
    "rating_mean = cereals_df.rating.mean()\n",
    "plot_1.quiver(calories_mean, rating_mean, *eigenvectors[:,0], color=['g'], scale=3)\n",
    "plot_1.quiver(calories_mean, rating_mean, *eigenvectors[:,1], color=['r'], scale=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d5ffc9-1cf9-4487-9699-19a8f4399624",
   "metadata": {},
   "source": [
    "Unser zweidimensionaler Datensatz liefert uns aus seiner Kovarianzmatrix (s. o.) zwei *Eigenvektoren*. Deren Merkmale können wir hier ausnutzen. Denn wir suchen zu unseren ursprünglichen Variablen $X_{calories}$ und $X_{rating}$ neue Variablen $PC_{1}$ und $PC_{2}$, die *gewichtete* Durchschnitte der ursprünglichen Werte abzüglich des Mittelwertes sind, so dass $PC_{1}$ und $PC_{2}$ *unkorreliert* sind. Z. B. für $PC_{1} = a_{1,1}(X_{calories}-\\overline{X}_{calories})+a_{1,2}(X_{rating}-\\overline{X}_{rating})$. Dann sortieren wir diese Linearkombinationen $PC$ der ursprünglichen $X$ nach ihrer Varianz. \n",
    "\n",
    "Die dazu passenden Gewichte $a_{i,j}$ erhalten wir aus den Eigenvektoren. Im Diagramm oben liegt der erste (grüne) Eigenvektor auf der Richtung der stärksten Varianz und damit der höchsten Information, mit einem höheren Eigenwert von 498.02 als der zweite (rote) Eigenvektor mit 78.93. Der liegt auf der Richtung der zweitstärksten Varianz und ist dabei orthogonal zum bzw. linear unabhängig vom ersten Eigenvektor - anschaulich steht er senkrecht darauf. Tatsächlich repräsentieren die beiden Eigenvektoren die gesuchten *Principal Components* bzw. Hauptkomponenten. Mit ihnen projizieren wir unseren ursprünglichen Datenraum auf ein neues Koordinatensystem, in dem die einzelnen Dimensionen, für uns also praktisch die Variablen mit ihren Daten, linear unkorreliert sind, wie wir es benötigen.\n",
    "\n",
    "Es gibt spezifische, effiziente Methoden der Implementierung einer PCA. Und mit zusätzlicher Hilfe von *scikit-learn* ist sie für unser Beispiel schnell durchgeführt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d057948-a9c1-4269-8070-3e75ee8970b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "pca.fit(cereals_df[['calories', 'rating']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c9da52-c719-4c90-9249-314233426010",
   "metadata": {},
   "source": [
    "Das `PCA`-Objekt kapselt für uns die einzelnen oben beschriebenen Eigen-Operationen, iteriert sie über die Variablenmenge mit ihren Daten und sortiert die Ergebnisse der Hauptkomponenten nach absteigender Varianz/Information. Schauen wir uns die Details an, indem wir die Resultate der PCA nach ihrer Ausführung etwas aufbereiten. Wir können verschiedene ihrer Elemente abfragen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431adfea-58f6-420c-89d4-215cdf205b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_summary = pcaSummary(pca)\n",
    "pca_summary = pca_summary.transpose()\n",
    "pca_summary.columns = ['PC1', 'PC2']\n",
    "print(pca_summary.to_string()+'\\n')\n",
    "pca_components = pd.DataFrame(pca.components_.transpose(),columns=['PC1', 'PC2'], index=['calories', 'rating'])\n",
    "print(pca_components.to_string()+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1dd9aa-7f7f-4a07-8e80-d97ed02feb53",
   "metadata": {},
   "source": [
    "Unsere Hauptkomponenten und damit die transformierten Daten besitzen nun jeweils eine Varianz von 498.02 und 78.93. Die Werte kommen uns bekannt vor: Es sind die Eigenwerte von oben. In Summe zeigt der transformierte Datensatz die gleiche Varianz von 576.96, wie das Original. Der Anteil der ersten Variable $PC1$ an der Gesamtvarianz beträgt nun aber über 86%, die zweite Variable steuert also nur noch etwas über 13% an Informationen bei.\n",
    "\n",
    "Um die umgewandelten Daten auch nutzen zu können, projizieren wir den ursprünglichen Datensatz mit Hilfe der Vektoren in `pca.components_` auf das neue Koordinatensystem: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24904cf-112e-423f-bf13-9cbc426723fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "projected_values = pd.DataFrame(pca.transform(cereals_df[['calories', 'rating']]), columns=['PC1', 'PC2'])\n",
    "projected_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565e897d-1762-4f8f-9966-90887667849a",
   "metadata": {},
   "source": [
    "Überprüfen wir noch die Korrelation der neuen Variablen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90f09dc-9048-4c94-a77c-a07e59eb687b",
   "metadata": {},
   "outputs": [],
   "source": [
    "projected_values.corr().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4d14ec-cfe0-43dc-9ffb-e81b3fb926b1",
   "metadata": {},
   "source": [
    "Gegenüber unserem Ausgangspunkt oben hätten wir uns verbessert, wenn wir uns nun auf die Betrachtung nur der Variable $PC1$ beschränken würden. Statt 34% würden nur noch 13% an Informationen unberücksichtigt bleiben.\n",
    "\n",
    "Die Anwendung der PCA mit *scikit-learn* ist schnell implementiert. Also dehnen wir sie auf den gesamten Datensatz aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22516a65-1fed-4ab2-9e9c-e0524ed307ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_full = PCA()\n",
    "# iloc entfernt hier die vorderen drei Spalten, dropna (*n*ot *a*vailable) Zeilen (axis=0) mit fehlenden Werten:\n",
    "pca_full.fit(cereals_df.iloc[:, 3:].dropna(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ba8959-5dfb-49ea-9372-8383324b01b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_full_summary = pcaSummary(pca_full)\n",
    "pca_full_summary = pca_full_summary.transpose()\n",
    "pca_full_summary.columns = [f'PC{i}' for i in range(1, len(pca_full_summary.columns)+1)]\n",
    "pca_full_summary.round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e38943b-2323-446a-bfe9-a1b1d6ed04c2",
   "metadata": {},
   "source": [
    "Laut der Auswertung der PCA über alle 13 Komponenten decken die ersten drei bereits über 96% der Informationen ab. Zehn Variablen könnten wir also ohne Probleme unberücksichtigt lassen. Oder? Werfen wir einen Blick auf die Elemente der Eigenvektoren, also die Gewichte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a46733-01c3-4247-b6b1-90bc0c61436b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_full_components = pd.DataFrame(pca_full.components_.transpose(),\n",
    "                                   columns=pca_full_summary.columns,\n",
    "                                   index=cereals_df.iloc[:, 3:].columns)\n",
    "pca_full_components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0072292-0942-49a5-a961-f82cc1af5f1f",
   "metadata": {},
   "source": [
    "Die Gewichte zeigen, wie stark jede der ursprünglichen Variablen die unterschiedlichen Hauptkomponenten beeinflusst. Das gibt uns einen Eindruck über die Struktur der untersuchten Daten.\n",
    "\n",
    "Im gegebenen Fall sehen wir, dass die erste Hauptkomponente sehr stark durch den Anteil an Sodium im jeweiligen Produkt bestimmt wird, praktisch den Sodium-Gehalt misst. Die zweite Hauptkomponente misst dafür das Kalium (potassium). Hier müssen wir beachten, dass diese beiden Anteile in Milligramm, die anderen jedoch in Gramm angegeben werden. Entsprechend sind die Varianzen von Sodium und Kalium im Verhältnis sehr hoch und ihr Anteil an der Gesamtvarianz damit auch (vollziehen Sie das z. B. über `cereals_df.cov()` nach).\n",
    "\n",
    "**Das ist ein kleines, aber wichtiges Beispiel dafür, dass wir die Daten, deren Eigenschaften im Detail und die Ergebnisse all der schönen, so einfach mal eben einsetzbaren Verfahren stets genauer prüfen sollten.**\n",
    "\n",
    "Die *Normalisierung, Normierung* bzw. *Standardisierung* (werden in der Literatur synonym verwendet) ist eine Möglichkeit, mit solchen Verzerrungen umzugehen. Eine Normierung subtrahiert den Mittelwert von allen Variablen bzw. deren Werten und teilt das Ergebnis durch die Standardabweichung. So bekommen alle Variablen die gleiche Relevanz bezüglich ihrer Informationen.\n",
    "\n",
    "Mathematisch bedeutet das, dass wir die Hauptkomponenten als Eigenvektoren nicht aus der *Kovarianz*matrix bestimmen (s. o.), sondern aus der *Korrelations*matrix, die ja relative Zusammenhänge angibt. Praktisch ergänzen wir in der PCA einfach `preprocessing.scale` auf die Daten, die wir `fit` übergeben:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3483bf0-0105-407b-ab86-7762ea41c41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_normalized = PCA()\n",
    "pca_normalized.fit(preprocessing.scale(cereals_df.iloc[:, 3:].dropna(axis=0)))\n",
    "pca_normalized_summary = pcaSummary(pca_normalized)\n",
    "pca_normalized_summary = pca_normalized_summary.transpose()\n",
    "pca_normalized_summary.columns = [f'PC{i}' for i in range(1, len(pca_normalized_summary.columns)+1)]\n",
    "pca_normalized_summary.round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac5ff7f-4d9d-487e-bdd7-fd82c2d45423",
   "metadata": {},
   "source": [
    "Nun benötigen wir sieben Komponenten, um über 90% der Informationen zu erhalten. Die Gewichte sind aber balancierter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fcbbca-7b0b-49a8-86af-1954c32f3cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_normalized_components = pd.DataFrame(pca_normalized.components_.transpose(),\n",
    "                                         columns=pca_normalized_summary.columns,\n",
    "                                         index=cereals_df.iloc[:, 3:].columns)\n",
    "pca_normalized_components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9138e707-eec9-414f-b308-94e0aee6c0d3",
   "metadata": {},
   "source": [
    "Die erste Komponente z. B. wägt nun ab zwischen Kalorien und Portionen (cups) auf der einen Seite mit negativen Gewichten, und Proteinen, Ballaststoffen (fiber), Kalium und der Produktbewertung mit positiven Gewichten auf der anderen Seite.\n",
    "\n",
    "Es gibt keine formal strikten Regeln, wann wir Daten normalisieren sollten. Für eine Heuristik stellen sich z. B. folgende Fragen:\n",
    "* Sind die Daten in der gleichen Einheit erfasst? Euro, Kilogramm, Liter, Stunden etc.?\n",
    "* Entsprechen die Skalierungen der Relevanz der Daten? Bei Beträgen z. B. große Diskrepanz zwischen Gewinn pro Aktie und Gesamtumsatz, bei zeitlichen Abläufen z. B. Millisekunden gegenüber Monaten."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9999861c-e257-4dbd-99d4-82122dc91f61",
   "metadata": {},
   "source": [
    "## Aufgabe\n",
    "\n",
    "Führen Sie die PCA *auf Basis von Eigenvektoren* für das Beispiel Kalorien und Bewertung *in normierter Form* durch. Stellen Sie die einzelnen Schritte wie oben dar. Hilfe zum Vorgehen:\n",
    "\n",
    "1. Wie gewohnt die Daten einlesen, dann aber mit `preprocessing.scale` die Daten normieren.\n",
    "2. Die normierten Daten von *calories* und *rating* mit ihren Eigenvektoren darstellen.\n",
    "3. Mit einer `PCA` und `fit` prüfen, ob die Eigenvektoren aus 2 stimmen (und sich im Graphen nicht durch die Richtungen irritieren lassen :-)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
