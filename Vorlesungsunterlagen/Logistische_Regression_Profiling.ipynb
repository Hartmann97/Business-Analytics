{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36ab8ed4-7305-4507-a9fa-7cb64b2fc063",
   "metadata": {},
   "source": [
    "#### Business Analytics FHDW 2025\n",
    "# Ein Beispiel zur logistischen Regression\n",
    "## Interpretation des Modells zum Profiling von Bankkunden\n",
    "\n",
    "Wir betrachten noch einmal den Datensatz *UniversalBank.csv*. Dieses Mal wollen wir weitere Prädiktoren berücksichtigen und die Ergebnisse in Richtung *Profiling* interpretieren.\n",
    "\n",
    "Den Datensatz bereiten wir uns dazu etwas auf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc15c3c-347e-4803-8271-d472c612df54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "from dmba import classificationSummary, gainsChart, liftChart\n",
    "\n",
    "bank_df = pd.read_csv('./Daten/UniversalBank.csv')\n",
    "# Die Daten der Variablen ID und PLZ entfernen wir (im Original/inplace):\n",
    "bank_df.drop(columns=['ID', 'ZIP Code'], inplace=True)\n",
    "bank_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d61345b-1cc3-4824-8a49-edeb7b518b5b",
   "metadata": {},
   "source": [
    "Die Variable *Ausbildung* (*Education*) ist im Datensatz als Integer kodiert. Daraus machen wir kategorische Dummy-Variablen und fassen dabei die drei bisher möglichen Werte zu zweien zusammen, um Multikollinearität zu vermeiden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75924f7f-e8d9-4ff5-b78d-d70af084c314",
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_df['Education'] = bank_df['Education'].astype('category')\n",
    "new_categories = {1: 'Undergrad', 2: 'Graduate', 3: 'Advanced'}\n",
    "bank_df['Education'] = bank_df.Education.cat.rename_categories(new_categories)\n",
    "bank_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9da4737-587d-4b69-b0c9-bc46c31417c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_df = pd.get_dummies(bank_df, prefix_sep='_', drop_first=True)\n",
    "bank_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dad381-0a3e-4356-9b66-9990fd960730",
   "metadata": {},
   "source": [
    "*CCAvg* sind die durchschnittlichen Ausgaben über Kreditkarte pro Monat, *SecuritiesAccount* zeigt an, ob ein Wertpapierdepot bei der Bank besteht, *CDAccount*, ob ein Bankeinlagenkonto (certificate of deposit) besteht, *Online* bezieht sich auf die Nutzung von Internet Banking.\n",
    "\n",
    "Auf dieser Basis können wir nun den Datensatz aufteilen und die logistische Regression durchführen. Wir wollen alle Variablen außer (natürlich) der Zielkategorie als Prädiktoren verwenden.\n",
    "\n",
    "Ohne Vertiefung an dieser Stelle zu den gegebenen Parametern der Regression: `penalty` und `C` dienen der *Regularisierung* gegen *overfitting*. Das Modell bekommt hier Funktionen und Werte vorgegeben, mit denen Ausreißer, Verzerrungen oder zu starke Eigenarten, von denen wir annehmen, dass sie spezifisch für die Trainingsdaten sind, \"bestraft\" und damit geglättet werden. Die Klasse bietet außerdem unterschiedliche Optimierungsalgorithmen an, die je nach Umfang und Eigenschaften der Datensätze mit `solver` ausgewählt werden können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c56a00-ac54-4fa2-9fe7-6ea294a7dcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = bank_df['PersonalLoan']\n",
    "X = bank_df.drop(columns=['PersonalLoan'])\n",
    "\n",
    "train_X, valid_X, train_y, valid_y = train_test_split(X, y, test_size=0.4, random_state=1)\n",
    "\n",
    "log_reg = LogisticRegression(penalty='l2', C=1e42, solver='liblinear')\n",
    "log_reg.fit(train_X, train_y)\n",
    "\n",
    "print('intercept ', log_reg.intercept_[0])\n",
    "coefficients = pd.DataFrame({'coeff': log_reg.coef_[0]}, index=X.columns)\n",
    "print(coefficients.transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7437a7-ed52-47a2-9640-71c144133afe",
   "metadata": {},
   "source": [
    "Um das theoretische Fundament nicht außen vor zu lassen, hier ausführlich das, was wir nun aufgestellt haben. Unser Vektor aus\n",
    "zwölf Prädiktoren ist \n",
    "\n",
    "$X=(Age, Experience, Income, Family, CCAvg, Mortgage, SecuritiesAccount, CDAccount, Online, CreditCard, Education\\_Graduate, Education\\_Advanced)$ \n",
    "\n",
    "Mit den ermittelten Werten oben ergibt sich der \n",
    "\n",
    "$Logit(PersonalLoan=Yes|X) = -12.1052-0.0517 Age + 0.053 Experience + 0.0587 Income + 0.61 Family + 0.2404 CCAvg + 0.001 Mortgage - 1.0301 \\underline{SecuritiesAccount} + 3.6482 \\underline{CDAccount} - 0.6789 \\underline{Online} - 0.9611 \\underline{CreditCard} + 4.1987 \\underline{Education\\_Graduate} + 4.3588 \\underline{Education\\_Advanced}$\n",
    "\n",
    "Die unterstrichenen Variablen sind binär, also *ja/1* oder *nein/0*.\n",
    "\n",
    "Aus den einzelnen Werten können wir bereits Tendenzen für die Kundenprofile ablesen. Je nach Vorzeichen und Größe beeinflusst der jeweilige Wert die Klassifizierung eines Kunden und damit seine Wahrscheinlichkeit, ein Darlehen zu akzeptieren. Ein Einlagenkonto und höherer Bildungsabschluss z. B. sind verbunden mit einer höheren Wahrscheinlichkeit. Ein Wertpapierdepot, Online Banking und eine Kreditkarte vermindern die Bereitschaft für das Darlehen.\n",
    "\n",
    "Interessant für die Profilbildung ist aber auch die Frage nach den konkreten, also quantifizierten Einflüssen der einzelnen Prädiktoren: Z. B. um welchen Wert ändert sich die Ziel-Wahrscheinlichkeit, wenn sich das Einkommen um eine Einheit erhöht? Das können wir direkt ablesen, wenn wir die *Odds* betrachten.\n",
    "\n",
    "Das liegt darin begründet, dass sich das Verhältnis zwischen den $Odds$ für einen Wert $x_i$ und $x_i+1$ auf eine Konstante reduzieren lässt (alle $x_j$ mit $j\\neq i$ bleiben unverändert):\n",
    "\n",
    "$\\frac{Odds(x_1,...,x_i+1,...,x_n)}{Odds(x_1,...,x_i,...,x_n)} = \\frac{e^{\\beta_0+\\beta_1 x_1+...+\\beta_i(x_i+1)+...+\\beta_n x_n}}{e^{\\beta_0+\\beta_1 x_1+...+\\beta_ix_i+...+\\beta_n x_n}} = e^{\\beta_i}$\n",
    "\n",
    "Bei einer Berechnung des Verhältnisses auf Basis der Wahrscheinlichkeiten würde das $x_i$ bestehen bleiben, das Ergebnis bei Steigerung um eine Einheit also immer auch von der jeweiligen Größe der Prädiktorvariable abhängig bleiben. \n",
    "\n",
    "Im gegebenen Beispiel wird also jeder Geburtstag eines Kunden die Chance, dass er ein Darlehen in Anspruch nimmt, um den Faktor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91add3a7-15b6-440c-8232-80c425ab2eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(coefficients['coeff']['Age'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6b8983-872f-4b5d-8413-7e1089c6d2ec",
   "metadata": {},
   "source": [
    "senken (wenn alle anderen Prädiktoren unverändert bleiben). Im Falle eines binären Prädiktors ist die Chance, dass z. B. eine Kundin mit einem fortgeschrittenen Bildungsabschluss ein Darlehen in Anspruch nimmt, um "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808abfb9-82d8-4fa3-985c-539b4196dd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(coefficients['coeff']['Education_Advanced'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb414e16-d9a5-4b5c-8f16-5883c2a45881",
   "metadata": {},
   "source": [
    "höher gegenüber einem Kunden ohne einen solchen Abschluss.\n",
    "\n",
    "*Was sind eigentlich solche theoretischen Betrachtungen in der Praxis wert? Wieso sind die dort heute relevant?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0e5eb2-f513-49fb-b096-9789fea2b521",
   "metadata": {},
   "source": [
    "## Performance der Klassifikation\n",
    "\n",
    "Betrachten wir noch die Qualität der prognostizierten Klassifikationen von Bankkunden. Für den Testdatensatz ermitteln wir dazu die Klassifikationen (*default cutoff* von 0.5) und deren Wahrscheinlichkeiten, bzw. *propensities*, Neigungen. Dazu können wir uns die Konfusionsmatrix anzeigen lassen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628a42f6-ad70-4cd7-9343-b6d66c5a62e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_predictions = log_reg.predict(valid_X)\n",
    "log_reg_probabilities = log_reg.predict_proba(valid_X)\n",
    "\n",
    "classificationSummary(valid_y, log_reg_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76d3f69-2065-43b6-9b8c-6f0542e1c825",
   "metadata": {},
   "source": [
    "Zwei weitere hilfreiche Diagramme sind die kumulierten Zuwächse (*cumulative gains chart*) und die relativen Steigerungen (*lift chart*). Zur Vorbereitung erstellen wir eine Liste mit den Wahrscheinlichkeiten aller Datenpunkte aus dem Validierungsdatensatz. Diese sortieren wir absteigend nach den Erfolgswahrscheinlichkeiten von Ereignis 1 bzw. hier der Annahme eines Darlehens. Die \"potentiell erfolgreichsten\" Datensätze stehen also oben in der Liste. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af970a3-c7a8-493a-989b-89c10055f0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_result = pd.DataFrame({'tatsächlich': valid_y,\n",
    "                               'p(0)': [p[0] for p in log_reg_probabilities],\n",
    "                               'p(1)': [p[1] for p in log_reg_probabilities],\n",
    "                               'vorhergesagt': log_reg_predictions})\n",
    "result_sorted = log_reg_result.sort_values(by=['p(1)'], ascending=False)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10,4))\n",
    "gainsChart(result_sorted.tatsächlich, ax= axes[0])\n",
    "liftChart(result_sorted.tatsächlich, title=False, ax= axes[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83dee9c-7891-4303-907d-29ff258e48d6",
   "metadata": {},
   "source": [
    "Das Gains-Diagramm zeigt uns damit die Steigerungskurve von für die Prognose eingesetzten, auf die obige Weise geordneten Datensätzen (x-Achse) gegenüber einer zufälligen Auswahl von Datensätzen (gestrichelte Linie). Praktisch für das Beispiel interpretiert: Die Kunden, die laut unserer logistischen Regression die höchsten Wahrscheinlichkeiten aufweisen, sind am lohnendsten für ein Darlehensangebot und sollten zuerst angesprochen werden. Im Lift-Diagramm sehen wir entsprechend, dass die obersten 10% dieser sortierten Liste fast 8-fach erhöhte Erfolgsaussichten gegenüber einer zufälligen 10%-Auswahl besitzen. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3774e62c-378e-45f6-8052-f16bb2428245",
   "metadata": {},
   "source": [
    "## Aufgabe\n",
    "\n",
    "Eine Personalberatung untersucht die Einflüsse der Erfahrung und Ausbildung von Systemadmins auf deren Fähigkeit, bestimmte Aufgaben in begrenzter Zeit zu lösen. Die Daten von 75 Admins finden sich in `SystemAdministrators.csv`: Die Variable *Experience* zeigt die Vollzeitbeschäftigung als Admin in Monaten, *Training* erworbene, fachlich relevante Credit Points, die Zielvariable *Completed task* zeigt, ob die Aufgaben in der verfügbaren Zeit gelöst wurden.\n",
    "\n",
    "1. Generieren Sie einen Scatterplot, der (z. B. farblich) illustriert, wer die Aufgaben gelöst hat und wer nicht. Welcher Prädiktor ist potentiell verwendbar für eine Vorhersage der erfolgreichen Aufgabenlösung?\n",
    "\n",
    "2. Implementieren Sie ein logistisches Regressionsmodell mit beiden Prädiktoren, das den gesamten Datensatz für das Training nutzt. Was ist der Prozentsatz der erfolgreichen Admins, die fälschlich als gescheitert eingeordnet werden, wenn Sie die Trainingsdaten auch für die Vorhersage verwenden?\n",
    "\n",
    "3. Sollten Sie den *cutoff* höher oder niedriger setzen, um den Prozentsatz aus 2 zu senken?\n",
    "\n",
    "4. Wie viele Monate Erfahrung muss ein Admin mit 4 Credits besitzen, damit die Erfolgswahrscheinlichkeit für die zeitige Aufgabenlösung 0.5 erreicht?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
